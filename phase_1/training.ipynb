{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "3c6a0be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "870e7869",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PDFPlumberLoader\n",
    "\n",
    "# loader = PDFPlumberLoader(\"../docs/Multimodal Retrieval.pdf\")\n",
    "loader = PDFPlumberLoader(\"../docs/Multimodal Retrieval.pdf\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "d41c83e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "\n",
    "chunks = splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "14819bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-large\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "362e5bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "c6a9df66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_embeddings(text):\n",
    "    response = client.embeddings.create(\n",
    "        input=text,\n",
    "        model=\"text-embedding-3-large\"\n",
    "    )\n",
    "    embeddings = np.array(response.data[0].embedding,dtype=np.float32)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e86d804",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "7a8394b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split the document into 6 chunks\n"
     ]
    }
   ],
   "source": [
    "import logfire\n",
    "import faiss\n",
    "from typing import List\n",
    "index = faiss.IndexFlatIP(3072)\n",
    "documents:List[dict] = []\n",
    "print(f\"Split the document into {len(chunks)} chunks\")\n",
    "embedddings_list =[]\n",
    "for i,chunk in enumerate(chunks):\n",
    "        response = get_embeddings(chunk.page_content)\n",
    "        documents.append({\n",
    "            \"text\":chunk,\n",
    "            \"filename\":\"basic-text.pdf\",\n",
    "            \"chunk_index\":i,\n",
    "                }\n",
    "        )\n",
    "        embedddings_list.append(response)\n",
    "    \n",
    "embedddings_matrix = np.vstack(embedddings_list).astype(np.float32)\n",
    "index.add(embedddings_matrix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "3e10917a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _retrive(question:str,top_k:int=3)->List[dict]:\n",
    "    if index.ntotal==0:\n",
    "      return []\n",
    "    with logfire.span(\"retrival of the chunks \",question=question):\n",
    "      question_embedding =get_embeddings(question)\n",
    "      query_vector = question_embedding.reshape(1,-1)\n",
    "      k = min(top_k,index.ntotal)\n",
    "      distances,indices = index.search(query_vector,k)\n",
    "      logfire.info(\"FAISS Search completed\")\n",
    "      top_chunks = []\n",
    "      for i,idx in enumerate(indices[0]) :\n",
    "        if idx<len(documents) and idx >=0:\n",
    "          chunk = documents[idx].copy()\n",
    "          chunk['similarity_score'] = float(distances[0][i])\n",
    "          top_chunks.append(chunk)\n",
    "      logfire.info(\"retivesd\")\n",
    "      return top_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "00f510c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _split_text(self,text:str)->List[str]:\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start <len(text):\n",
    "      end = start + self.chunk_size\n",
    "      chunk = text[start:end]\n",
    "      if end < len(text):\n",
    "        last_period = chunk.rfind(\".\")\n",
    "        if last_period > self.chunk_size//2:\n",
    "          chunk = chunk[:last_period+1]\n",
    "          end = start+last_period+1\n",
    "      chunks.append(chunk.strip())\n",
    "      start = end-self.chunk_overlap\n",
    "    return [c for c in chunks if c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26799b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple,List\n",
    "def ask(question:str)->Tuple[str,List[str]]:\n",
    "    with logfire.span(\"rag_ask\",question=question):\n",
    "      relevant_chunks = _retrive(question)\n",
    "      if not relevant_chunks:\n",
    "        return (\n",
    "\n",
    "            \"dont have relevant chunks\"\n",
    "            \"please upload another document\",\n",
    "            []\n",
    "\n",
    "        )\n",
    "      # print(relevant_chunks)\n",
    "    context = \"\\n\\n\".join([chunk for chunk in relevant_chunks])\n",
    "\n",
    "    system_propmt = \"\"\"you are a helpful assistant that answer question based in the provided contextt,\n",
    "    Rules:\n",
    "    1.only use information form the provided context\n",
    "    2.If the context doenst contain the answer say so\n",
    "    3.Be consise and accurate\n",
    "    4.cite which document the information came from\"\"\"\n",
    "    user_prompt =f\"\"\" Context:{context} Question:{question}\n",
    "    please answer the question based on the context above\"\"\"\n",
    "    logfire.info(\"Generating the answer with openai\")\n",
    "    response =client.chat.completions.create(\n",
    "        model=\"gpt-4o-min\",\n",
    "        messages=[\n",
    "            {\"role\":\"system\",\"content\":system_propmt},\n",
    "            {\"role\":\"user\",\"content\":user_prompt}\n",
    "        ]\n",
    "    )\n",
    "    answer = response.choices[0].message.content\n",
    "    sources = list(set(chunk[\"filename\"] for chunk in relevant_chunks))\n",
    "    logfire.info(\"Answer generated\")\n",
    "    return answer,sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "4d1c4bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'text': Document(metadata={'source': '../docs/Multimodal Retrieval.pdf', 'file_path': '../docs/Multimodal Retrieval.pdf', 'page': 0, 'total_pages': 6, 'Author': '', 'CreationDate': 'D:20251130052107Z', 'Creator': 'Nebo', 'ModDate': 'D:20251130052107Z', 'Producer': 'MyScript interactive ink', 'Title': 'Multimodal Retrieval'}, page_content='Custom Training of CLIP.\\nEncoder Tent Encooler\\nImage\\n① LSTM.\\n① Remit.\\n•\\nI'), 'filename': 'basic-text.pdf', 'chunk_index': 0, 'similarity_score': 0.22641685605049133}, {'text': Document(metadata={'source': '../docs/Multimodal Retrieval.pdf', 'file_path': '../docs/Multimodal Retrieval.pdf', 'page': 1, 'total_pages': 6, 'Author': '', 'CreationDate': 'D:20251130052107Z', 'Creator': 'Nebo', 'ModDate': 'D:20251130052107Z', 'Producer': 'MyScript interactive ink', 'Title': 'Multimodal Retrieval'}, page_content='☒ I m a g e P u p o u r i n g\\n> R u b e I m a g e .\\nI m a g e\\n( 2 2 4 × 2 2 4 × 3 )\\n( h x w x c )\\nB l a ck\\nA s p e c t R o l o . ( p a d d ed\\n0 s ) .\\nw it h\\n✓\\nI m g s h ap e → 2 2 4 ✗ m a - 3 → 3 × 2 2 4 × 2 2 4 .\\nIt w .\\nW C C\\nH\\nI / P : 3 1 2 2 4 × 2 2 4 5 12 × 2 ✗ 2\\n10 2 4 ×\\n× I\\n8 × 1 1 2 x 1 / 2\\n10 2 4 .\\n5 6\\n5 6 ×\\n16 ×\\n2 8\\n3 2 × 2 8\\n14\\n6 4 × 1 4\\n1 2 8 × ✗ 7\\n7\\n2 5 6 × ×\\n3 3'), 'filename': 'basic-text.pdf', 'chunk_index': 1, 'similarity_score': 0.15664660930633545}, {'text': Document(metadata={'source': '../docs/Multimodal Retrieval.pdf', 'file_path': '../docs/Multimodal Retrieval.pdf', 'page': 2, 'total_pages': 6, 'Author': '', 'CreationDate': 'D:20251130052107Z', 'Creator': 'Nebo', 'ModDate': 'D:20251130052107Z', 'Producer': 'MyScript interactive ink', 'Title': 'Multimodal Retrieval'}, page_content='BX3X 224×224\\nRemit\\n> 13×5127768/1025\\n☐ Tent.\\n> [\\n]\\n① Get all captions of Train Set\\n② for c in caption:\\n③ all-words • enteral (c. shut C) )\\n(all-wires)).\\n④ unique-wires = lint (net\\n⑤ unique wires. with\\n⑥ unique_ was = [\" (Bos)\" ,\"LEOS?\", K PAD)\"\\n\" unus\"]!\\nunique_woes.\\n✓ ✓\\n⑦ word 2 int a j int sword\\n⑧ for word in enumerate (unique-words):\\n⑨ word 2 int [word ] = i\\nint 2 word [i ] = word.\\n⑩\\nCREATION.\\nVOCAB'), 'filename': 'basic-text.pdf', 'chunk_index': 2, 'similarity_score': 0.15007320046424866}]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "sequence item 0: expected str instance, dict found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[131], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m answer, _ \u001b[38;5;241m=\u001b[39m \u001b[43mask\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat is Multimodal Retrival?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m answer\n",
      "Cell \u001b[1;32mIn[130], line 14\u001b[0m, in \u001b[0;36mask\u001b[1;34m(question)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m      7\u001b[0m \n\u001b[0;32m      8\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdont have relevant chunks\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m \n\u001b[0;32m     12\u001b[0m     )\n\u001b[0;32m     13\u001b[0m   \u001b[38;5;28mprint\u001b[39m(relevant_chunks)\n\u001b[1;32m---> 14\u001b[0m context \u001b[38;5;241m=\u001b[39m \u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrelevant_chunks\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m system_propmt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124myou are a helpful assistant that answer question based in the provided contextt,\u001b[39m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;124mRules:\u001b[39m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124m1.only use information form the provided context\u001b[39m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;124m2.If the context doenst contain the answer say so\u001b[39m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;124m3.Be consise and accurate\u001b[39m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;124m4.cite which document the information came from\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     22\u001b[0m user_prompt \u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m Context:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Question:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquestion\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;124mplease answer the question based on the context above\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n",
      "\u001b[1;31mTypeError\u001b[0m: sequence item 0: expected str instance, dict found"
     ]
    }
   ],
   "source": [
    "answer, _ = ask(\"What is Multimodal Retrival?\")\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0a2fc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
