{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "3c6a0be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "870e7869",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PDFPlumberLoader\n",
    "\n",
    "# loader = PDFPlumberLoader(\"../docs/Multimodal Retrieval.pdf\")\n",
    "loader = PDFPlumberLoader(\"../docs/Capstone Requirements - Phase 1 Review.pdf\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "d41c83e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "\n",
    "chunks = splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "14819bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-large\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "362e5bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "c6a9df66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_embeddings(text):\n",
    "    response = client.embeddings.create(\n",
    "        input=text,\n",
    "        model=\"text-embedding-3-large\"\n",
    "    )\n",
    "    embeddings = np.array(response.data[0].embedding,dtype=np.float32)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b52163",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "7a8394b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split the document into 5 chunks\n"
     ]
    }
   ],
   "source": [
    "import logfire\n",
    "import faiss\n",
    "from typing import List\n",
    "index = faiss.IndexFlatIP(3072)\n",
    "documents:List[dict] = []\n",
    "print(f\"Split the document into {len(chunks)} chunks\")\n",
    "embedddings_list =[]\n",
    "for i,chunk in enumerate(chunks):\n",
    "        response = get_embeddings(chunk.page_content)\n",
    "        documents.append({\n",
    "            \"text\":chunk,\n",
    "            \"filename\":\"Project Proposal_ Multimodal Media Retrieval.pdf\",\n",
    "            \"chunk_index\":i,\n",
    "                }\n",
    "        )\n",
    "        embedddings_list.append(response)\n",
    "    \n",
    "embedddings_matrix = np.vstack(embedddings_list).astype(np.float32)\n",
    "index.add(embedddings_matrix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "3e10917a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _retrive(question:str,top_k:int=3)->List[dict]:\n",
    "    if index.ntotal==0:\n",
    "      return []\n",
    "    with logfire.span(\"retrival of the chunks \",question=question):\n",
    "      question_embedding =get_embeddings(question)\n",
    "      query_vector = question_embedding.reshape(1,-1)\n",
    "      k = min(top_k,index.ntotal)\n",
    "      distances,indices = index.search(query_vector,k)\n",
    "      logfire.info(\"FAISS Search completed\")\n",
    "      top_chunks = []\n",
    "      for i,idx in enumerate(indices[0]) :\n",
    "        if idx<len(documents) and idx >=0:\n",
    "          chunk = documents[idx].copy()\n",
    "          chunk['similarity_score'] = float(distances[0][i])\n",
    "          top_chunks.append(chunk)\n",
    "      logfire.info(\"retivesd\")\n",
    "      return top_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "9061561b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _retrive(question: str, top_k: int = 5):\n",
    "    question_embedding = get_embeddings(question)  # shape: (D,)\n",
    "\n",
    "    # FAISS expects (1, D)\n",
    "    query_vector = np.array([question_embedding]).astype(\"float32\")\n",
    "\n",
    "    k = min(top_k, index.ntotal)\n",
    "\n",
    "    distances, indices = index.search(query_vector, k)\n",
    "\n",
    "    top_chunks = []\n",
    "    for idx in indices[0]:\n",
    "        if idx != -1:\n",
    "            top_chunks.append(chunks[idx])\n",
    "\n",
    "    return top_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "00f510c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _split_text(self,text:str)->List[str]:\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start <len(text):\n",
    "      end = start + self.chunk_size\n",
    "      chunk = text[start:end]\n",
    "      if end < len(text):\n",
    "        last_period = chunk.rfind(\".\")\n",
    "        if last_period > self.chunk_size//2:\n",
    "          chunk = chunk[:last_period+1]\n",
    "          end = start+last_period+1\n",
    "      chunks.append(chunk.strip())\n",
    "      start = end-self.chunk_overlap\n",
    "    return [c for c in chunks if c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "26799b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple,List\n",
    "def ask(question:str)->Tuple[str,List[str]]:\n",
    "    with logfire.span(\"rag_ask\",question=question):\n",
    "      relevant_chunks = _retrive(question)\n",
    "      if not relevant_chunks:\n",
    "        return (\n",
    "\n",
    "            \"dont have relevant chunks\"\n",
    "            \"please upload another document\",\n",
    "            []\n",
    "\n",
    "        )\n",
    "      print(relevant_chunks)\n",
    "    context = \"\\n\\n\".join([chunk.page_content for chunk in relevant_chunks])\n",
    "\n",
    "    system_propmt = \"\"\"you are a helpful assistant that answer question based in the provided contextt,\n",
    "    Rules:\n",
    "    1.only use information form the provided context\n",
    "    2.If the context doenst contain the answer say so\n",
    "    3.Be consise and accurate\n",
    "    4.cite which document the information came from\"\"\"\n",
    "    user_prompt =f\"\"\" Context:{context} Question:{question}\n",
    "    please answer the question based on the context above\"\"\"\n",
    "    logfire.info(\"Generating the answer with openai\")\n",
    "    response =client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\":\"system\",\"content\":system_propmt},\n",
    "            {\"role\":\"user\",\"content\":user_prompt}\n",
    "        ]\n",
    "    )\n",
    "    answer = response.choices[0].message.content\n",
    "    sources = list(set(chunk.page_content for chunk in relevant_chunks))\n",
    "    logfire.info(\"Answer generated\")\n",
    "    return answer,sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "4d1c4bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': '../docs/Capstone Requirements - Phase 1 Review.pdf', 'file_path': '../docs/Capstone Requirements - Phase 1 Review.pdf', 'page': 0, 'total_pages': 1, 'Title': 'Capstone Requirements', 'Producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext', 'Author': 'Seshadri Mazumder', 'Creator': 'Notes', 'CreationDate': \"D:20251220022827Z00'00'\", 'ModDate': \"D:20251220022827Z00'00'\"}, page_content='Capstone Requirements\\nDear Teams,\\nThese are the following requirements we have to showcase:\\n1. Pretrained CLIP Inference\\n1. Flickr8K\\n1. Image2Text - Quantitative & Qualitative both\\n2. Text2Image - Quantitative & Qualitative both\\n3. Text2Text - Quantitative & Qualitative both\\n4. Image2Image - Qualitative only\\n2. Flickr30K\\n1. Image2Text - Quantitative & Qualitative both\\n2. Text2Image - Quantitative & Qualitative both\\n3. Text2Text - Quantitative & Qualitative both\\n4. Image2Image - Qualitative only'), Document(metadata={'source': '../docs/Capstone Requirements - Phase 1 Review.pdf', 'file_path': '../docs/Capstone Requirements - Phase 1 Review.pdf', 'page': 0, 'total_pages': 1, 'Title': 'Capstone Requirements', 'Producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext', 'Author': 'Seshadri Mazumder', 'Creator': 'Notes', 'CreationDate': \"D:20251220022827Z00'00'\", 'ModDate': \"D:20251220022827Z00'00'\"}, page_content='3. Text2Text - Quantitative & Qualitative both\\n4. Image2Image - Qualitative only\\n5. Training & Testing/Validation Loss Per Epoch - IMP\\n2. Flickr30K\\n1. Image2Text - Quantitative & Qualitative both\\n2. Text2Image - Quantitative & Qualitative both\\n3. Text2Text - Quantitative & Qualitative both\\n4. Image2Image - Qualitative only\\n5. Training & Testing/Validation Loss Per Epoch - IMP\\nPlease put everything in slides, Please verify the numbers within a reasonable range\\nfrom your fellow group mates.'), Document(metadata={'source': '../docs/Capstone Requirements - Phase 1 Review.pdf', 'file_path': '../docs/Capstone Requirements - Phase 1 Review.pdf', 'page': 0, 'total_pages': 1, 'Title': 'Capstone Requirements', 'Producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext', 'Author': 'Seshadri Mazumder', 'Creator': 'Notes', 'CreationDate': \"D:20251220022827Z00'00'\", 'ModDate': \"D:20251220022827Z00'00'\"}, page_content='from your fellow group mates.\\nDont put screenshots from colab Notebook containing code snippers, we trust you,\\nso whatever you put, put as tables and images directly.\\nWe should show 3 Models from the above for each group, let me know which all\\nthree models your groups will be showcasing, because some will be interested in\\nchanging the Image encoder first whereas some will be interested in changing the\\ntext encoder first.'), Document(metadata={'source': '../docs/Capstone Requirements - Phase 1 Review.pdf', 'file_path': '../docs/Capstone Requirements - Phase 1 Review.pdf', 'page': 0, 'total_pages': 1, 'Title': 'Capstone Requirements', 'Producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext', 'Author': 'Seshadri Mazumder', 'Creator': 'Notes', 'CreationDate': \"D:20251220022827Z00'00'\", 'ModDate': \"D:20251220022827Z00'00'\"}, page_content='3. Text2Text - Quantitative & Qualitative both\\n4. Image2Image - Qualitative only\\n⸻⸻⸻⸻⸻⸻⸻⸻⸻⸻⸻—\\nModel Designs:\\nModel Name Image Encoder Text Encoder\\nM1 Resnet LSTM\\nM2 Resnet LSTM + Attention\\nM3 VIT LSTM\\nM4 VIT LSTM + Attention\\nM5 VIT Transformer\\n2. CLIP Trained from Scratch\\n1. Flickr8K\\n1. Image2Text - Quantitative & Qualitative both\\n2. Text2Image - Quantitative & Qualitative both\\n3. Text2Text - Quantitative & Qualitative both\\n4. Image2Image - Qualitative only'), Document(metadata={'source': '../docs/Capstone Requirements - Phase 1 Review.pdf', 'file_path': '../docs/Capstone Requirements - Phase 1 Review.pdf', 'page': 0, 'total_pages': 1, 'Title': 'Capstone Requirements', 'Producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext', 'Author': 'Seshadri Mazumder', 'Creator': 'Notes', 'CreationDate': \"D:20251220022827Z00'00'\", 'ModDate': \"D:20251220022827Z00'00'\"}, page_content='text encoder first.\\nPlease continue rigorously with the experiments today and tomorrow first half as\\nwell.\\nTomorrow 2nd half post lunch I’ll be prof’s review.\\nLet me know if anything.')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The capstone requirements include the following:\\n\\n1. **Pretrained CLIP Inference**:\\n   - For **Flickr8K**:\\n     - Image2Text - Quantitative & Qualitative both\\n     - Text2Image - Quantitative & Qualitative both\\n     - Text2Text - Quantitative & Qualitative both\\n     - Image2Image - Qualitative only\\n   - For **Flickr30K**:\\n     - Image2Text - Quantitative & Qualitative both\\n     - Text2Image - Quantitative & Qualitative both\\n     - Text2Text - Quantitative & Qualitative both\\n     - Image2Image - Qualitative only\\n   - Training & Testing/Validation Loss Per Epoch - Important (IMP)\\n\\n2. **Model Designs**:\\n   - Various Models with different designs specifying the image and text encoders.\\n\\n3. **CLIP Trained from Scratch**:\\n   - Requirements similar to those for Pretrained CLIP Inference, specifically for Flickr8K.\\n\\nAdditional instructions include creating slides, verifying numbers, avoiding code screenshots, and selecting three models to showcase.\\n\\n(Source: Capstone Requirements Document)'"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer, _ = ask(\"What are capstone requirements?\")\n",
    "answer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
