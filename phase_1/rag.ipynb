{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c1fdf62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from posix import stat\n",
    "# import logfire\n",
    "from openai import AsyncOpenAI\n",
    "from typing import List,Tuple\n",
    "import numpy as np\n",
    "import faiss\n",
    "import os\n",
    "import logfire\n",
    "# from app.config import settings\n",
    "\n",
    "class RAGEngine:\n",
    "  def __init__(self):\n",
    "    self.client = AsyncOpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "    self.documents:List[dict] = []\n",
    "    self.embedding_dim = 1536\n",
    "    self.index = faiss.IndexFlatIP(self.embedding_dim)\n",
    "    self.chunk_size = 500\n",
    "    self.chunk_overlap = 50\n",
    "    logfire.info(\"RAG Engine Initialised with FAISS\")\n",
    "\n",
    "  #step 1 index and process the documents\n",
    "\n",
    "  def _split_text(self,text:str)->List[str]:\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start <len(text):\n",
    "      end = start + self.chunk_size\n",
    "      chunk = text[start:end]\n",
    "      if end < len(text):\n",
    "        last_period = chunk.rfind(\".\")\n",
    "        if last_period > self.chunk_size//2:\n",
    "          chunk = chunk[:last_period+1]\n",
    "          end = start+last_period+1\n",
    "      chunks.append(chunk.strip())\n",
    "      start = end-self.chunk_overlap\n",
    "    return [c for c in chunks if c]\n",
    "\n",
    "  async def _get_embeddings(self,text:str)->np.array:\n",
    "    with logfire.span(\"get_embeddings\"):\n",
    "      response = await self.client.embeddings.create(\n",
    "        input=text,\n",
    "        model=\"text-embedding-3-small\")\n",
    "      embeddings = np.array(response.data[0].embedding,dtype=np.float32)\n",
    "      return embeddings\n",
    "  async def add_documents(self,text:str,filename:str)->int:\n",
    "    with logfire.span(\"add_document\",filename=filename):\n",
    "      chunks = self._split_text(text)\n",
    "      logfire.info(f\"Split the document into {len(chunks)} chunks\")\n",
    "      embedddings_list =[]\n",
    "      for i,chunk in enumerate(chunks):\n",
    "        embedding = await self._get_embeddings(chunk)\n",
    "        embedddings_list.append(embedding)\n",
    "        self.documents.append({\n",
    "            \"text\":chunk,\n",
    "            \"filename\":filename,\n",
    "            \"chunk_index\":i,\n",
    "        }\n",
    "\n",
    "        )\n",
    "\n",
    "      embedddings_matrix = np.vstack(embedddings_list).astype(np.float32)\n",
    "      self.index.add(embedddings_matrix)\n",
    "      logfire.info(f\"Added {len(chunks)} chunks to FAISS VB\")\n",
    "      return len(chunks)\n",
    "\n",
    "  async def _retrive(self,question:str,top_k:int=3)->List[dict]:\n",
    "    if self.index.ntotal==0:\n",
    "      return []\n",
    "    with logfire.span(\"retrival of the chunks \",question=question):\n",
    "      question_embedding =await self._get_embeddings(question)\n",
    "      query_vector = question_embedding.reshape(-1,1)\n",
    "      k = min(top_k,self.index.ntotal)\n",
    "      distances,indices = self.index.search(query_vector,k)\n",
    "      logfire.info(\"FAISS Search completed\")\n",
    "      top_chunks = []\n",
    "      for i,idx in enumerate(indices[0]) :\n",
    "        if idx<len(self.documents) and idx >=0:\n",
    "          chunk = self.documents[idx].copy()\n",
    "          chunk['similarity_score'] = float(distances[0][i])\n",
    "          top_chunks.append(chunk)\n",
    "      logfire.info(\"retivesd\")\n",
    "      return top_chunks\n",
    "\n",
    "  async def ask(self,question:str)->Tuple[str,List[str]]:\n",
    "    with logfire.span(\"rag_ask\",question=question):\n",
    "      relevant_chunks = await self._retrive(question)\n",
    "      if not relevant_chunks:\n",
    "        return (\n",
    "\n",
    "            \"dont have relevant chunks\"\n",
    "            \"please upload another document\",\n",
    "            []\n",
    "\n",
    "        )\n",
    "    context = \"\\n\\n\".join([chunk for chunk in relevant_chunks])\n",
    "\n",
    "    system_propmt = \"\"\"you are a helpful assistant that answer question based in the provided contextt,\n",
    "    Rules:\n",
    "    1.only use information form the provided context\n",
    "    2.If the context doenst contain the answer say so\n",
    "    3.Be consise and accurate\n",
    "    4.cite which document the information came from\"\"\"\n",
    "    user_prompt =f\"\"\" Context:{context} Question:{question}\n",
    "    please answer the question based on the context above\"\"\"\n",
    "    logfire.info(\"Generating the answer with openai\")\n",
    "    response = await self.client.chat.completions.create(\n",
    "        model=\"gpt-4o-min\",\n",
    "        messages=[\n",
    "            {\"role\":\"system\",\"content\":system_propmt},\n",
    "            {\"role\":\"user\",\"content\":user_prompt}\n",
    "        ]\n",
    "    )\n",
    "    answer = response.choices[0].message.content\n",
    "    sources = list(set(chunk[\"filename\"] for chunk in relevant_chunks))\n",
    "    logfire.info(\"Answer generated\")\n",
    "    return answer,sources\n",
    "  def list_documents(self)-> List[dict]:\n",
    "    seen = set()\n",
    "    docs = []\n",
    "    for doc in self.documents:\n",
    "      if doc[\"filename\"] not in seen:\n",
    "        seen.add(doc[\"filename\"])\n",
    "        chunk_count = sum(1 for chunk in self.documents if chunk[\"filename\"]==doc[\"filename\"])\n",
    "        doc.append(\n",
    "            {\n",
    "                \"chunk_count\":chunk_count,\n",
    "                \"file_name\":doc[\"filename\"]\n",
    "            }\n",
    "        )\n",
    "      return docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6d1a2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ragengine = RAGEngine()\n",
    "from langchain.document_loaders import PDFPlumberLoader\n",
    "\n",
    "# loader = PDFPlumberLoader(\"../docs/Multimodal Retrieval.pdf\")\n",
    "loader = PDFPlumberLoader(\"../docs/basic-text.pdf\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1baa1147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Chunks: 1\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2000,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "\n",
    "chunks = splitter.split_documents(documents)\n",
    "print(f\"Total Chunks: {len(chunks)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d64328a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_1664\\3148111319.py:1: RuntimeWarning: coroutine 'RAGEngine.add_documents' was never awaited\n",
      "  ragengine.add_documents(chunks,\"basic-text.pdf\")\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_1664\\3148111319.py:2: RuntimeWarning: coroutine 'RAGEngine.ask' was never awaited\n",
      "  ans, _ =ragengine.ask(\"What is document about?\")\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable coroutine object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m ragengine\u001b[38;5;241m.\u001b[39madd_documents(chunks,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbasic-text.pdf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m ans, _ \u001b[38;5;241m=\u001b[39mragengine\u001b[38;5;241m.\u001b[39mask(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is document about?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable coroutine object"
     ]
    }
   ],
   "source": [
    "ragengine.add_documents(chunks,\"basic-text.pdf\")\n",
    "ans, _ =ragengine.ask(\"What is document about?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
